{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "11a4327e",
      "metadata": {
        "id": "11a4327e"
      },
      "source": [
        "# Dynamic Evaluation with trace collection\n",
        "\n",
        "* Difference with **static** evaluation: dynamic evaluation add trace collection on real GPU devices.\n",
        "* Usage:\n",
        "  * On Colab, Select CPU as Runtime, Click `Runtime -> Run All`\n",
        "  * On other environment, first create a virturalenv and `Select Kernel` to use the virtualenv kernel, then click `Runtime -> Run All`\n",
        "* Hardware Requirement: a NVIDIA GPU with the CUDA driver installed. Please note:\n",
        "  1. The choice of hardware will significantly affect results. Please consider using A100, the same hardware we used in the paper.\n",
        "  2. Please make sure no other workload is executing on the same GPU.\n",
        "* Software Requirement: NEUTRINO system only depends on GNU toolchain (gcc, file, git, nm), CUDA toolchain (cuobjdump, ptxas) and Python 3.12 (pip, toml). But evaluation workload needs a PTX- included build of PyTorch and CUTLASS. We package the dependency checking and installation in prepare_env.py for one-click installation.\n",
        "* Disk Space Requirement: Please arrange at least 10GB for collecting traces.\n",
        "\n",
        "Expected results:\n",
        "Dynamic evaluation on customized traces is expected to produce similar results, i.e., similar numbers or figure shapes. And if you encountered any problems, please contact us through HotCRP.\n",
        "\n",
        "Notes:\n",
        "Each following section can be executed independently after downloading the code and setting up the environment."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0. Prepare Environment\n",
        "\n",
        "Prepare environment takes two steps:\n",
        "1. Download the packaged code from anonymoused R2 object storage\n",
        "2. Prepare the environment with [`test_env.py`](https://github.com/neutrino-gpu/neutrino/blob/artifact/artifact/prepare_env.py)"
      ],
      "metadata": {
        "id": "0JtmcA_6Ulnc"
      },
      "id": "0JtmcA_6Ulnc"
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://pub-eef24bf0aa5b4950860ea28dfbe39d8c.r2.dev/dynamic.zip\n",
        "!unzip dynamic.zip"
      ],
      "metadata": {
        "id": "y9Mro7gGUow_"
      },
      "id": "y9Mro7gGUow_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "!wget https://github.com/neutrino-gpu/neutrino/blob/artifact/artifact/prepare_env.py\n",
        "!{sys.executable} prepare_env.py"
      ],
      "metadata": {
        "id": "dnHMspTOVswG"
      },
      "id": "dnHMspTOVswG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Block Scheduling Cost\n",
        "\n",
        "> We recommend you start with this as Hello World Example\n",
        "\n",
        "* Correspondence to paper: Sec. 4.5\n",
        "* Notes: Please complete this in \"Kick-the-tires\" run. This help justifying correct environment setup."
      ],
      "metadata": {
        "id": "AafFlXQkWgh8"
      },
      "id": "AafFlXQkWgh8"
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile block_sched/memset.py\n",
        "import torch\n",
        "torch.zeros((4096,4096), dtype=torch.float16, device=\"cuda\")"
      ],
      "metadata": {
        "id": "C8GvzpEjWxTL"
      },
      "id": "C8GvzpEjWxTL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Then run the toolkit to check the result\n",
        "!neutrino -p block_sched --tracedir block_sched python block_sched/raw.py"
      ],
      "metadata": {
        "id": "gRoAxqiyXAm-"
      },
      "id": "gRoAxqiyXAm-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Block Scheduling Cost Optimization\n",
        "\n",
        "As stated in Sec.4.5, there's two way to optimize the cost and gain performance improvement:\n",
        "1. CUDA Driver's `memset` API\n",
        "2. Persistent Kernel with Triton, written by Cursor backed by GPT-4o"
      ],
      "metadata": {
        "id": "sr7ar-B5XNyr"
      },
      "id": "sr7ar-B5XNyr"
    },
    {
      "cell_type": "code",
      "source": [
        "# First let's measure the time of raw implementation via Nsight System\n",
        "!nsys nvprof python block_sched/raw.py"
      ],
      "metadata": {
        "id": "B6H7tZuxXGUF"
      },
      "id": "B6H7tZuxXGUF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile block_sched/memset.py\n",
        "# Code for using CUDA Driver's memset\n",
        "from cuda.bindings import driver\n",
        "import torch\n",
        "\n",
        "def _cudaGetErrorEnum(error):\n",
        "    if isinstance(error, driver.CUresult):\n",
        "        err, name = driver.cuGetErrorName(error)\n",
        "        return name if err == driver.CUresult.CUDA_SUCCESS else \"<unknown>\"\n",
        "    else:\n",
        "        raise RuntimeError('Unknown error type: {}'.format(error))\n",
        "\n",
        "def checkCudaErrors(result):\n",
        "    if result[0].value:\n",
        "        raise RuntimeError(\"CUDA error code={}({})\".format(result[0].value, _cudaGetErrorEnum(result[0])))\n",
        "    if len(result) == 1:\n",
        "        return None\n",
        "    elif len(result) == 2:\n",
        "        return result[1]\n",
        "    else:\n",
        "        return result[1:]\n",
        "\n",
        "# Initialize CUDA Driver API\n",
        "checkCudaErrors(driver.cuInit(0))\n",
        "\n",
        "# Retrieve handle for device 0\n",
        "cuDevice = checkCudaErrors(driver.cuDeviceGet(0))\n",
        "\n",
        "for _ in range(100): # Driver API is not stable, need more runs to get correct value\n",
        "    a = torch.empty((4096, 4096), dtype=torch.float16, device=\"cuda\")\n",
        "    checkCudaErrors(driver.cuMemsetD16(a.data_ptr(), 0, 4096*4096))"
      ],
      "metadata": {
        "id": "1Uu0CWOTXSx6"
      },
      "id": "1Uu0CWOTXSx6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nsys nvprof python block_sched/raw.py"
      ],
      "metadata": {
        "id": "NLG2KJr_XUZl"
      },
      "id": "NLG2KJr_XUZl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile block_sched/persist.py\n",
        "# Persistent Kernel Implementation\n",
        "# Acknowledgement: This is written by Cursor backed by GPT-4o\n",
        "import torch\n",
        "import triton\n",
        "import triton.language as tl\n",
        "\n",
        "@triton.jit\n",
        "def zero_init_kernel_persistent(\n",
        "    output_ptr,\n",
        "    numel,\n",
        "    BLOCK_SIZE: tl.constexpr,\n",
        "    NUM_SMS: tl.constexpr\n",
        "):\n",
        "    # Get program ID\n",
        "    start_pid = tl.program_id(axis=0)\n",
        "    # Calculate number of blocks needed\n",
        "    num_blocks = tl.cdiv(numel, BLOCK_SIZE)\n",
        "    # Calculate blocks per SM\n",
        "    blocks_per_sm = num_blocks // NUM_SMS\n",
        "    if start_pid < num_blocks % NUM_SMS:\n",
        "        blocks_per_sm += 1\n",
        "    # Initialize block ID\n",
        "    block_id = start_pid - NUM_SMS\n",
        "\n",
        "    # Process multiple blocks per SM\n",
        "    for _ in range(blocks_per_sm):\n",
        "        block_id += NUM_SMS\n",
        "        # Calculate offsets for this block\n",
        "        offsets = block_id * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n",
        "        # Create mask for valid elements\n",
        "        mask = offsets < numel\n",
        "        # Store zeros\n",
        "        tl.store(output_ptr + offsets, tl.zeros([BLOCK_SIZE], dtype=tl.float16), mask=mask)\n",
        "\n",
        "def zero_init_persistent(x):\n",
        "    # Get total number of elements\n",
        "    numel = x.numel()\n",
        "    # Get number of SMs\n",
        "    NUM_SMS = torch.cuda.get_device_properties(\"cuda\").multi_processor_count\n",
        "    # Configure BLOCK_SIZE still at 128\n",
        "    BLOCK_SIZE = 128\n",
        "    # Launch kernel\n",
        "    grid = lambda META: (min(NUM_SMS, triton.cdiv(numel, META['BLOCK_SIZE'])),)\n",
        "    zero_init_kernel_persistent[grid](\n",
        "        x,\n",
        "        numel,\n",
        "        BLOCK_SIZE=BLOCK_SIZE,\n",
        "        NUM_SMS=NUM_SMS,\n",
        "        num_warps=8\n",
        "    )\n",
        "\n",
        "# NOTE replace with torch.empty to only allocate without initialize\n",
        "a = torch.empty((4096, 4096), dtype=torch.float16, device=\"cuda\")\n",
        "for _ in range(100):\n",
        "    zero_init_persistent(a)"
      ],
      "metadata": {
        "id": "tKgcTgx7XVy5"
      },
      "id": "tKgcTgx7XVy5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To measure the time, we can use the Nsight System\n",
        "!nsys nvprof python block_sched/persist.py"
      ],
      "metadata": {
        "id": "nuvDAA6DXfa_"
      },
      "id": "nuvDAA6DXfa_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Densified Memory Access Timeline (DMAT) Plot\n",
        "\n",
        "* Correspondence to paper: Fig.1, Fig.10(A/B/C), in total 4 figures presented\n",
        "* Hardware: A100 GPU\n",
        "* Notes:\n",
        "  1. DMAT takes time, please be patient\n",
        "  2. Each run of will yield slightly different results\n",
        "  3. DMAT varies on different hardware, you will need to use an RTX3080 to have exactly the same results as presented\n",
        "* Others: We will use two [Jupyter Magic](https://ipython.readthedocs.io/en/stable/interactive/magics.html#) to help evaluation\n",
        "  * `%%writefile` will write the cell to path, *noted that the code won't be executed*, we use it to save code as Neutrino is a CLI tool.\n",
        "  * `%%capture` will capture the `stdout` of the cell, we use it to read the path of generated DMAT Plot for display."
      ],
      "metadata": {
        "id": "WJLjSxRJZeDc"
      },
      "id": "WJLjSxRJZeDc"
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile dmat/common.py\n",
        "# Common Kernel of Fig.1 and Fig.10A\n",
        "import torch\n",
        "import triton\n",
        "import triton.language as tl\n",
        "from neutrino.utils.tensortrace import TensorTrace\n",
        "\n",
        "@triton.jit\n",
        "def _attn_fwd_inner(acc, l_i, m_i, q,  #\n",
        "                    K_block_ptr, V_block_ptr,  #\n",
        "                    start_m, qk_scale,  #\n",
        "                    BLOCK_M: tl.constexpr, HEAD_DIM: tl.constexpr, BLOCK_N: tl.constexpr,  #\n",
        "                    STAGE: tl.constexpr, offs_m: tl.constexpr, offs_n: tl.constexpr,  #\n",
        "                    N_CTX: tl.constexpr, fp8_v: tl.constexpr):\n",
        "    # range of values handled by this stage\n",
        "    if STAGE == 1:\n",
        "        lo, hi = 0, start_m * BLOCK_M\n",
        "    elif STAGE == 2:\n",
        "        lo, hi = start_m * BLOCK_M, (start_m + 1) * BLOCK_M\n",
        "        lo = tl.multiple_of(lo, BLOCK_M)\n",
        "    # causal = False\n",
        "    else:\n",
        "        lo, hi = 0, N_CTX\n",
        "    K_block_ptr = tl.advance(K_block_ptr, (0, lo))\n",
        "    V_block_ptr = tl.advance(V_block_ptr, (lo, 0))\n",
        "    # loop over k, v and update accumulator\n",
        "    for start_n in range(lo, hi, BLOCK_N):\n",
        "        start_n = tl.multiple_of(start_n, BLOCK_N)\n",
        "        # -- compute qk ----\n",
        "        k = tl.load(K_block_ptr)\n",
        "        qk = tl.dot(q, k)\n",
        "        if STAGE == 2:\n",
        "            mask = offs_m[:, None] >= (start_n + offs_n[None, :])\n",
        "            qk = qk * qk_scale + tl.where(mask, 0, -1.0e6)\n",
        "            m_ij = tl.maximum(m_i, tl.max(qk, 1))\n",
        "            qk -= m_ij[:, None]\n",
        "        else:\n",
        "            m_ij = tl.maximum(m_i, tl.max(qk, 1) * qk_scale)\n",
        "            qk = qk * qk_scale - m_ij[:, None]\n",
        "        p = tl.math.exp2(qk)\n",
        "        l_ij = tl.sum(p, 1)\n",
        "        # -- update m_i and l_i\n",
        "        alpha = tl.math.exp2(m_i - m_ij)\n",
        "        l_i = l_i * alpha + l_ij\n",
        "        # -- update output accumulator --\n",
        "        acc = acc * alpha[:, None]\n",
        "        # update acc\n",
        "        v = tl.load(V_block_ptr)\n",
        "        if fp8_v:\n",
        "            p = p.to(tl.float8e5)\n",
        "        else:\n",
        "            p = p.to(tl.float16)\n",
        "        acc = tl.dot(p, v, acc)\n",
        "        # update m_i and l_i\n",
        "        m_i = m_ij\n",
        "        V_block_ptr = tl.advance(V_block_ptr, (BLOCK_N, 0))\n",
        "        K_block_ptr = tl.advance(K_block_ptr, (0, BLOCK_N))\n",
        "    return acc, l_i, m_i\n",
        "\n",
        "@triton.jit\n",
        "def _attn_fwd(Q, K, V, sm_scale, M, Out,  #\n",
        "              stride_qz, stride_qh, stride_qm, stride_qk,  #\n",
        "              stride_kz, stride_kh, stride_kn, stride_kk,  #\n",
        "              stride_vz, stride_vh, stride_vk, stride_vn,  #\n",
        "              stride_oz, stride_oh, stride_om, stride_on,  #\n",
        "              Z, H, N_CTX,  #\n",
        "              HEAD_DIM: tl.constexpr,  #\n",
        "              BLOCK_M: tl.constexpr,  #\n",
        "              BLOCK_N: tl.constexpr,  #\n",
        "              STAGE: tl.constexpr  #\n",
        "              ):\n",
        "    tl.static_assert(BLOCK_N <= HEAD_DIM)\n",
        "    start_m = tl.program_id(0)\n",
        "    off_hz = tl.program_id(1)\n",
        "    off_z = off_hz // H\n",
        "    off_h = off_hz % H\n",
        "    qvk_offset = off_z.to(tl.int64) * stride_qz + off_h.to(tl.int64) * stride_qh\n",
        "\n",
        "    # block pointers\n",
        "    Q_block_ptr = tl.make_block_ptr(\n",
        "        base=Q + qvk_offset,\n",
        "        shape=(N_CTX, HEAD_DIM),\n",
        "        strides=(stride_qm, stride_qk),\n",
        "        offsets=(start_m * BLOCK_M, 0),\n",
        "        block_shape=(BLOCK_M, HEAD_DIM),\n",
        "        order=(1, 0),\n",
        "    )\n",
        "    v_order: tl.constexpr = (0, 1) if V.dtype.element_ty == tl.float8e5 else (1, 0)\n",
        "    V_block_ptr = tl.make_block_ptr(\n",
        "        base=V + qvk_offset,\n",
        "        shape=(N_CTX, HEAD_DIM),\n",
        "        strides=(stride_vk, stride_vn),\n",
        "        offsets=(0, 0),\n",
        "        block_shape=(BLOCK_N, HEAD_DIM),\n",
        "        order=v_order,\n",
        "    )\n",
        "    K_block_ptr = tl.make_block_ptr(\n",
        "        base=K + qvk_offset,\n",
        "        shape=(HEAD_DIM, N_CTX),\n",
        "        strides=(stride_kk, stride_kn),\n",
        "        offsets=(0, 0),\n",
        "        block_shape=(HEAD_DIM, BLOCK_N),\n",
        "        order=(0, 1),\n",
        "    )\n",
        "    O_block_ptr = tl.make_block_ptr(\n",
        "        base=Out + qvk_offset,\n",
        "        shape=(N_CTX, HEAD_DIM),\n",
        "        strides=(stride_om, stride_on),\n",
        "        offsets=(start_m * BLOCK_M, 0),\n",
        "        block_shape=(BLOCK_M, HEAD_DIM),\n",
        "        order=(1, 0),\n",
        "    )\n",
        "    # initialize offsets\n",
        "    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n",
        "    offs_n = tl.arange(0, BLOCK_N)\n",
        "    # initialize pointer to m and l\n",
        "    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")\n",
        "    l_i = tl.zeros([BLOCK_M], dtype=tl.float32) + 1.0\n",
        "    acc = tl.zeros([BLOCK_M, HEAD_DIM], dtype=tl.float32)\n",
        "    # load scales\n",
        "    qk_scale = sm_scale\n",
        "    qk_scale *= 1.44269504  # 1/log(2)\n",
        "    # load q: it will stay in SRAM throughout\n",
        "    q = tl.load(Q_block_ptr)\n",
        "    # stage 1: off-band\n",
        "    # For causal = True, STAGE = 3 and _attn_fwd_inner gets 1 as its STAGE\n",
        "    # For causal = False, STAGE = 1, and _attn_fwd_inner gets 3 as its STAGE\n",
        "    if STAGE & 1:\n",
        "        acc, l_i, m_i = _attn_fwd_inner(acc, l_i, m_i, q, K_block_ptr, V_block_ptr,  #\n",
        "                                        start_m, qk_scale,  #\n",
        "                                        BLOCK_M, HEAD_DIM, BLOCK_N,  #\n",
        "                                        4 - STAGE, offs_m, offs_n, N_CTX, V.dtype.element_ty == tl.float8e5  #\n",
        "                                        )\n",
        "    # stage 2: on-band\n",
        "    if STAGE & 2:\n",
        "        # barrier makes it easier for compielr to schedule the\n",
        "        # two loops independently\n",
        "        acc, l_i, m_i = _attn_fwd_inner(acc, l_i, m_i, q, K_block_ptr, V_block_ptr,  #\n",
        "                                        start_m, qk_scale,  #\n",
        "                                        BLOCK_M, HEAD_DIM, BLOCK_N,  #\n",
        "                                        2, offs_m, offs_n, N_CTX, V.dtype.element_ty == tl.float8e5  #\n",
        "                                        )\n",
        "    # epilogue\n",
        "    m_i += tl.math.log2(l_i)\n",
        "    acc = acc / l_i[:, None]\n",
        "    m_ptrs = M + off_hz * N_CTX + offs_m\n",
        "    tl.store(m_ptrs, m_i)\n",
        "    tl.store(O_block_ptr, acc.to(Out.type.element_ty))\n",
        "\n",
        "def forward(q, k, v, causal, sm_scale, BLOCK_M, BLOCK_N, NUM_STAGES, NUM_WARPS):\n",
        "    # shape constraints\n",
        "    HEAD_DIM_Q, HEAD_DIM_K = q.shape[-1], k.shape[-1]\n",
        "    # when v is in float8_e5m2 it is transposed.\n",
        "    HEAD_DIM_V = v.shape[-1]\n",
        "    assert HEAD_DIM_Q == HEAD_DIM_K and HEAD_DIM_K == HEAD_DIM_V\n",
        "    assert HEAD_DIM_K in {16, 32, 64, 128, 256}\n",
        "    o = torch.empty_like(q)\n",
        "    stage = 3 if causal else 1\n",
        "    extra_kern_args = {}\n",
        "\n",
        "    grid = (triton.cdiv(q.shape[2], BLOCK_M), q.shape[0] * q.shape[1], 1)\n",
        "    M = torch.empty((q.shape[0], q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\n",
        "\n",
        "    # Actually prevent triton.autotune to take manual control\n",
        "    config = triton.Config({}, num_stages=NUM_STAGES, num_warps=NUM_WARPS)\n",
        "    _attn_fwd_tuned = triton.autotune(configs=[config, ], key=[])(_attn_fwd)\n",
        "\n",
        "    _attn_fwd_tuned[grid](\n",
        "            q, k, v, sm_scale, M, o,  #\n",
        "            q.stride(0), q.stride(1), q.stride(2), q.stride(3),  #\n",
        "            k.stride(0), k.stride(1), k.stride(2), k.stride(3),  #\n",
        "            v.stride(0), v.stride(1), v.stride(2), v.stride(3),  #\n",
        "            o.stride(0), o.stride(1), o.stride(2), o.stride(3),  #\n",
        "            q.shape[0], q.shape[1],  #\n",
        "            N_CTX=q.shape[2],  #\n",
        "            HEAD_DIM=HEAD_DIM_K,  #\n",
        "            BLOCK_M=BLOCK_M,\n",
        "            BLOCK_N=BLOCK_N\n",
        "            STAGE=stage,  #\n",
        "            **extra_kern_args)\n",
        "    return q, k, v, o, M, sm_scale, HEAD_DIM_K"
      ],
      "metadata": {
        "id": "bGVpdzcWZ0Wt"
      },
      "id": "bGVpdzcWZ0Wt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile dmat/fig1.py\n",
        "# Fig 1A\n",
        "import torch\n",
        "from neutrino.utils.tensortrace import TensorTrace\n",
        "from common import forward # defined above\n",
        "\n",
        "BLOCK_M, BLOCK_N, NUM_STAGES, NUM_WARPS = 128, 64, 2, 8\n",
        "\n",
        "torch.manual_seed(42)\n",
        "\n",
        "with TensorTrace() as t:\n",
        "    shape = (4, 32, 4096, 64) # batch_size, head_num, seq_len, head_dim\n",
        "    q = torch.empty(shape, dtype=torch.float16, device=\"cuda\").normal_(mean=0.0, std=0.5)\n",
        "    k = torch.empty(shape, dtype=torch.float16, device=\"cuda\").normal_(mean=0.0, std=0.5)\n",
        "    v = torch.empty(shape, dtype=torch.float16, device=\"cuda\").normal_(mean=0.0, std=0.5)\n",
        "    # dout = torch.randn_like(q) # a random gradient\n",
        "    causal = False\n",
        "    sm_scale = 0.5\n",
        "\n",
        "    forward(q, k, v, causal, sm_scale, BLOCK_M, BLOCK_N, NUM_STAGES, NUM_WARPS)"
      ],
      "metadata": {
        "id": "ORdLRHXuZ2sZ"
      },
      "id": "ORdLRHXuZ2sZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture cap --no-stderr # capture the stdout of this cell containing path to dmat.png\n",
        "!neutrino -t dmat -k attn python dmat/fig1.py # our kernel has attn keyword in naming for matching"
      ],
      "metadata": {
        "id": "IW4lGxdDZ6-5"
      },
      "id": "IW4lGxdDZ6-5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Image(cap.stdout)"
      ],
      "metadata": {
        "id": "5EqMIUjyaDNM"
      },
      "id": "5EqMIUjyaDNM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile dmat/fig10a.py\n",
        "# Fig. 10A code\n",
        "import torch\n",
        "from neutrino.utils.tensortrace import TensorTrace\n",
        "from common import forward # defined above\n",
        "\n",
        "BLOCK_M, BLOCK_N, NUM_STAGES, NUM_WARPS = 64, 64, 2, 4\n",
        "\n",
        "torch.manual_seed(42)\n",
        "\n",
        "with TensorTrace() as t:\n",
        "    shape = (4, 32, 4096, 64) # batch_size, head_num, seq_len, head_dim\n",
        "    q = torch.empty(shape, dtype=torch.float16, device=\"cuda\").normal_(mean=0.0, std=0.5)\n",
        "    k = torch.empty(shape, dtype=torch.float16, device=\"cuda\").normal_(mean=0.0, std=0.5)\n",
        "    v = torch.empty(shape, dtype=torch.float16, device=\"cuda\").normal_(mean=0.0, std=0.5)\n",
        "    # dout = torch.randn_like(q) # a random gradient\n",
        "    causal = False\n",
        "    sm_scale = 0.5\n",
        "\n",
        "    forward(q, k, v, causal, sm_scale, BLOCK_M, BLOCK_N, NUM_STAGES, NUM_WARPS)"
      ],
      "metadata": {
        "id": "x8YcfHm-aHgY"
      },
      "id": "x8YcfHm-aHgY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture cap --no-stderr # capture the stdout of this cell containing path to dmat.png\n",
        "!neutrino -t dmat -k attn python dmat/fig10a.py"
      ],
      "metadata": {
        "id": "-bQo92bjaIK1"
      },
      "id": "-bQo92bjaIK1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Image(cap.stdout)"
      ],
      "metadata": {
        "id": "nzKEnKgjaJk-"
      },
      "id": "nzKEnKgjaJk-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fig. 10B: FlashAttn-v1\n",
        "\n",
        "For FlashAttn-v1, we can use the official `scaled_dot_product_attention` provided by PyTorch.\n",
        "\n",
        "To prevent other implementation, please use `sdpa_kernel([SDPBackend.FLASH_ATTENTION])`, we follow the same setup as above:"
      ],
      "metadata": {
        "id": "HwPlRSSraE-N"
      },
      "id": "HwPlRSSraE-N"
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile dmat/fig10b.py\n",
        "import torch\n",
        "from neutrino.utils.tensortrace import TensorTrace\n",
        "\n",
        "torch.manual_seed(42)\n",
        "\n",
        "with TensorTrace() as t:\n",
        "    shape = (4, 32, 4096, 64) # batch_size, head_num, seq_len, head_dim\n",
        "    q = torch.empty(shape, dtype=torch.float16, device=\"cuda\").normal_(mean=0.0, std=0.5)\n",
        "    k = torch.empty(shape, dtype=torch.float16, device=\"cuda\").normal_(mean=0.0, std=0.5)\n",
        "    v = torch.empty(shape, dtype=torch.float16, device=\"cuda\").normal_(mean=0.0, std=0.5)\n",
        "    # dout = torch.randn_like(q) # a random gradient\n",
        "    causal = False\n",
        "    sm_scale = 0.5\n",
        "\n",
        "    with torch.nn.attention.sdpa_kernel([torch.nn.attention.SDPBackend.FLASH_ATTENTION, ]):\n",
        "        torch.nn.functional.scaled_dot_product_attention(q, k, v, is_causal=causal, scale=sm_scale)"
      ],
      "metadata": {
        "id": "qFKOxdNtaNTg"
      },
      "id": "qFKOxdNtaNTg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture cap --no-stderr # use Jupyter Magic to capture the stdout of this cell := path to dmat.png\n",
        "# Flash Attn Kernels are named as ...flash_fwd_kernel..., let's use flash to locate it\n",
        "!neutrino -t dmat -k flash python dmat/fig10b.py"
      ],
      "metadata": {
        "id": "44mlQtHyapYk"
      },
      "id": "44mlQtHyapYk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Image(cap.stdout) # display the"
      ],
      "metadata": {
        "id": "_vICxbsDarwF"
      },
      "id": "_vICxbsDarwF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fig. 10C: Memory Efficient Attention\n",
        "\n",
        "Similarly, Memory Efficient Attention are also provided via `scaled_dot_product_attention` of PyTorch.\n",
        "\n",
        "Please use `sdpa_kernel([SDPBackend.EFFICIENT_ATTENTION])` to specify."
      ],
      "metadata": {
        "id": "ZZdXLDneascI"
      },
      "id": "ZZdXLDneascI"
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile dmat/fig10c.py\n",
        "import torch\n",
        "from neutrino.utils.tensortrace import TensorTrace\n",
        "\n",
        "torch.manual_seed(42)\n",
        "\n",
        "with TensorTrace() as t:\n",
        "    shape = (4, 32, 4096, 64)\n",
        "    q = torch.empty(shape, dtype=torch.float16, device=\"cuda\").normal_(mean=0.0, std=0.5)\n",
        "    k = torch.empty(shape, dtype=torch.float16, device=\"cuda\").normal_(mean=0.0, std=0.5)\n",
        "    v = torch.empty(shape, dtype=torch.float16, device=\"cuda\").normal_(mean=0.0, std=0.5)\n",
        "    # dout = torch.randn_like(q) # a random gradient\n",
        "    causal = False\n",
        "    sm_scale = 0.5\n",
        "\n",
        "    with torch.nn.attention.sdpa_kernel([torch.nn.attention.SDPBackend.EFFICIENT_ATTENTION, ]):\n",
        "        torch.nn.functional.scaled_dot_product_attention(q, k, v, is_causal=causal, scale=sm_scale)"
      ],
      "metadata": {
        "id": "KMd6upFCauPd"
      },
      "id": "KMd6upFCauPd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture cap --no-stderr # use Jupyter Magic to capture the stdout of this cell := path to dmat.png\n",
        "# Memory Efficient Attention are written in CUTLASS named `fmha`\n",
        "!neutrino -t dmat -k fmha python dmat/fig10c.py"
      ],
      "metadata": {
        "id": "EewDt8zSavxG"
      },
      "id": "EewDt8zSavxG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Image(cap.stdout) # display the"
      ],
      "metadata": {
        "id": "MfuXChtNaw82"
      },
      "id": "MfuXChtNaw82",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Kernel Slowdown and Additional Registers - Part 1\n",
        "\n",
        "* Correspondence to paper: Table. 2\n",
        "* Remarks: Benchmarking mode must be enabled through `--benchmark` CLI option. This mode also disable result saving that reduce disk space usage.\n",
        "\n",
        "Our evaluation consists of three parts:\n",
        "1. CUTLASS Kernels: Standard GEMM, Stream-K GEMM, Conv2D\n",
        "2. Triton Kernels: Group MM,  FlashAttn-v2\n",
        "3. PyTorch Kernels: Batch/LayerNorm, SoftMax, Sum, Max/AvgPool, Embedding, Gather\n",
        "\n",
        "Evaluation Target:\n",
        "1. Kenrel Slowdown: `probed / pruned`.\n",
        "2. Additional Register Usage: `probed - pruned`"
      ],
      "metadata": {
        "id": "sdPHAasaayPd"
      },
      "id": "sdPHAasaayPd"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "\n",
        "# METRICS = [\"block_sched\", \"tensorop_count\", \"gmem_bytes\", \"dmat\"]\n",
        "METRICS = [\"dmat\"]\n",
        "# NOTE filter out distribution kernels (torch.randn) used to initialize test inputs\n",
        "FILTERED_KERNEL = \"distribution:copy\"\n",
        "\n",
        "# PyTorch op_benchmark\n",
        "TORCH_TESTS = [\"norm\", \"softmax\", \"sum\", \"pool\", \"embedding\", \"gather\"]\n",
        "\n",
        "for test in TORCH_TESTS:\n",
        "    os.makedirs(test, exist_ok=True)\n",
        "    for metric in METRICS:\n",
        "        os.makedirs(os.path.join(test, metric), exist_ok=True)\n",
        "        print(test, metric)\n",
        "        with open(os.path.join(test, metric, \"stdout.log\"), \"w\") as f:\n",
        "            res = subprocess.run(\n",
        "              [\"neutrino\", \"--benchmark\", \"-p\", f\"./{metric}.toml\", \"--filter\", FILTERED_KERNEL, sys.executable, \"-m\", f\"pt.{test}_test\"],\n",
        "              stderr=subprocess.PIPE, stdout=f)\n",
        "        tracedir = res.stderr.decode().split(\"\\n\")[0].strip().split(\" \")[-1]\n",
        "        try:\n",
        "            os.rename(tracedir, os.path.join(test, metric, \"trace\"))\n",
        "        except:\n",
        "            print(res.stderr.decode())\n",
        "\n",
        "# Triton and CUTLASS, modified from exposed_latency/main.py\n",
        "COMMANDS = {\n",
        "    \"attn\": [sys.executable, \"attn.py\",  \"4\", \"32\", \"4096\", \"64\", \"128\", \"64\", \"2\", \"4\"],\n",
        "    \"gmm\":  [sys.executable, \"gmm.py\", \"512\", \"512\", \"512\", \"512\"],\n",
        "    \"gemm\": [\"../cutlass/build/examples/benchmark/gemm\", \"--iterations=100\"],\n",
        "    \"gemm\": [\"../cutlass/build/examples/benchmark/gemm-streamk\", \"--iterations=100\"],\n",
        "    \"conv\": [\"../cutlass/build/examples/benchmark/conv\", \"--n=256\", \"--h=56\", \"--w=56\", \"--c=128\", \"--k=128\", \"--r=3\", \"--s=3\"]\n",
        "}\n",
        "\n",
        "for test, command in COMMANDS.items():\n",
        "    os.makedirs(test, exist_ok=True)\n",
        "    for metric in METRICS:\n",
        "        os.makedirs(os.path.join(test, metric), exist_ok=True)\n",
        "        print(test, metric)\n",
        "        with open(os.path.join(test, metric, \"stdout.log\"), \"w\") as f:\n",
        "            res = subprocess.run(\n",
        "                [\"neutrino\", \"--benchmark\", \"-p\", f\"./{metric}.toml\", \"--filter\", FILTERED_KERNEL, \"-k\", test, *command],\n",
        "                stderr=subprocess.PIPE, stdout=f)\n",
        "        tracedir = res.stderr.decode().strip().split(\" \")[-1]\n",
        "        os.rename(tracedir, os.path.join(test, metric, \"trace\"))"
      ],
      "metadata": {
        "id": "-5IztvVHbBm-"
      },
      "id": "-5IztvVHbBm-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Maximum Memory Usage\n",
        "\n",
        "* Correspondence to paper: Fig. 11, Sec. 6\n",
        "* Remarks:\n",
        "  1. Make sure NO other workload is running on the GPU. (use CUDA_VISIBLE_DEVICES to switch if need)\n",
        "  2. Here we're not directly evaluating the model but `torch.compile`d modules.\n",
        "  This is because PyTorch by default use cuBLAS as `nn.Linear` backend but cuBLAS is not tracable by Neutrino (due to dark apis),\n",
        "  and we can not neglect `nn.Linear`.\n",
        "  Thus, we use the `torch.compile` to enfore all kernels in PyTorch ATen/Triton for complete tracing and fair evaluation.\n",
        "  3. Profiling a whole model can be expensive (in time and speed), we recommend first use `--memusage` option to disable the real measurement and only measure the maximum meory usage for a test run.\n",
        "\n",
        "Evaluation Target: The Maximum Probe Memory Usage"
      ],
      "metadata": {
        "id": "Mc4ScXKYdgr8"
      },
      "id": "Mc4ScXKYdgr8"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import subprocess\n",
        "\n",
        "BASE = \"max_mem/compiled\"\n",
        "TESTS = [\"block_sched\", \"gmem_bytes\", \"tensorop_count\", \"dmat\"]\n",
        "MODELS = [\"resnet-bs32\", \"sd-bs1\", \"mamba-bs32\", \"llama1b-bs32\", \"llama3b-bs32\", \"llama8b-bs32\", \"llama8b-bs128\", \"llama8b-bs256\"]\n",
        "\n",
        "for model in MODELS:\n",
        "    os.makedirs(os.path.join(model), exist_ok=True)\n",
        "    for test in TESTS:\n",
        "        os.makedirs(os.path.join(model, test), exist_ok=True)\n",
        "        dirs = os.listdir(os.path.join(BASE, model))\n",
        "        for sub in dirs:\n",
        "            os.makedirs(os.path.join(model, test, sub[:-3]), exist_ok=True)\n",
        "            cmd = [\"neutrino\", \"-p\", test, \"--memusage\", \"python\", os.path.join(BASE, model, sub)]\n",
        "            print(\" \".join(cmd))\n",
        "            res = subprocess.run(cmd, stderr=subprocess.PIPE, stdout=subprocess.PIPE)\n",
        "            # print(res)\n",
        "            for tracedir in res.stderr.decode().split(\"\\n\"):\n",
        "                if len(tracedir) < 3: continue\n",
        "                # print(tracedir)\n",
        "                tracedir = tracedir.strip().split(\" \")[-1]\n",
        "                # print(tracedir)\n",
        "                shutil.copytree(tracedir, os.path.join(model, test, sub[:-3], os.path.basename(tracedir)))\n",
        "            with open(os.path.join(model, test, sub[:-3], \"stdout.log\"), \"w\") as f:\n",
        "                f.write(res.stdout.decode())"
      ],
      "metadata": {
        "id": "rJqZCWb4dtTf"
      },
      "id": "rJqZCWb4dtTf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Exposed Latency\n",
        "\n",
        "* Correspondence to paper: Fig. 12, Sec. 6\n",
        "* Remarks:\n",
        "  1. Make sure NO other workload is running on the GPU. (use CUDA_VISIBLE_DEVICES to switch if need)"
      ],
      "metadata": {
        "id": "Sjto_e0Pe2v1"
      },
      "id": "Sjto_e0Pe2v1"
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "!{sys.executable} exposed_latency/main.py"
      ],
      "metadata": {
        "id": "RnqgJKfRfZLf"
      },
      "id": "RnqgJKfRfZLf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Warp Scheduling and Tailing Effect\n",
        "\n",
        "* Correspondence to paper: Sec. 7\n",
        "* Remarks:\n",
        "  1. Each run of will yield slightly different results\n",
        "  2. Warp Scheduling varies on different hardware, you will need to use an A100 to have exactly the same results as presented."
      ],
      "metadata": {
        "id": "dRcg8emGe3jq"
      },
      "id": "dRcg8emGe3jq"
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import os\n",
        "from IPython.display import Image # For display"
      ],
      "metadata": {
        "id": "8eJHr3pKe_-f"
      },
      "id": "8eJHr3pKe_-f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run\n",
        "!cd warp_sched/ && {sys.executable} main.py"
      ],
      "metadata": {
        "id": "Q95shx97fB9F"
      },
      "id": "Q95shx97fB9F",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fig13a\n",
        "tracedir = os.path.join(\"warp_sched\", \"trace\", \"exclusive_sched\", \"result\")\n",
        "trace = os.path.join(tracedir, os.listdir(tracedir)[0])\n",
        "!{sys.executable} warp_sched/fig13a.py {trace}\n",
        "Image(\"warp_sched/fig13a.png\")"
      ],
      "metadata": {
        "id": "e3Itl751fEI-"
      },
      "id": "e3Itl751fEI-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fig13b\n",
        "tracedir = os.path.join(\"warp_sched\", \"trace\", \"shared_sched\", \"result\")\n",
        "trace = os.path.join(tracedir, os.listdir(tracedir)[0])\n",
        "!{sys.executable} warp_sched/fig13a.py {trace}\n",
        "Image(\"warp_sched/fig13b.png\")"
      ],
      "metadata": {
        "id": "guRnX3sbfF1-"
      },
      "id": "guRnX3sbfF1-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fig13cd\n",
        "tracedir = os.path.join(\"warp_sched\", \"trace\", \"shared_acc\", \"result\")\n",
        "trace = os.path.join(tracedir, os.listdir(tracedir)[0])\n",
        "!{sys.executable} warp_sched/fig13cd_raw.py {trace}"
      ],
      "metadata": {
        "id": "ODhYLYt6fIRd"
      },
      "id": "ODhYLYt6fIRd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Image(\"warp_sched/fig13c.png\")"
      ],
      "metadata": {
        "id": "fto5UuZUfJ1M"
      },
      "id": "fto5UuZUfJ1M",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Image(\"warp_sched/fig13d.png\")"
      ],
      "metadata": {
        "id": "YmFlFKyIfLHU"
      },
      "id": "YmFlFKyIfLHU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Image(\"warp_sched/fig13d-sub.png\")"
      ],
      "metadata": {
        "id": "KdjJK2ArfMZQ"
      },
      "id": "KdjJK2ArfMZQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Neutrino Internal Workflows\n",
        "\n",
        "> All results for reproducable are presented above, following contents introduces your Neutrino internal contents to help you judge the Artifact Functionality.\n",
        "\n",
        "Here we address three question:\n",
        "1. What is the source code of Neutrino probe to be inserted?\n",
        "2. How Neutrino raw trace looks like?\n",
        "3. What is the analyze code to finally print out result?"
      ],
      "metadata": {
        "id": "IRZRV8z5Xk_f"
      },
      "id": "IRZRV8z5Xk_f"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What's the probe code?\n",
        "\n",
        "We specify the probe with `-p/--probe` CLI option like `-p block_sched`, and these (built-in) probes are under the `tool` folder of Neutrino installation."
      ],
      "metadata": {
        "id": "AiMAfcRnXyjV"
      },
      "id": "AiMAfcRnXyjV"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import neutrino # as a python package, Neutrino can be imported!\n",
        "\n",
        "os.listdir(os.path.join(os.path.dirname(neutrino.__file__), \"tools\"))"
      ],
      "metadata": {
        "id": "DLUsISegXkxL"
      },
      "id": "DLUsISegXkxL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Neutrino takes probes as the `toml` file, and `block_sched` will be automatically matched to the `block_sched.toml` in the above folder.\n",
        "\n",
        "Moreover, we can further take a look at the content."
      ],
      "metadata": {
        "id": "w1_wn6taX20w"
      },
      "id": "w1_wn6taX20w"
    },
    {
      "cell_type": "code",
      "source": [
        "!cat {os.path.join(os.path.dirname(neutrino.__file__), \"tools\", \"block_sched.toml\")}"
      ],
      "metadata": {
        "id": "_jx5AO19X1XQ"
      },
      "id": "_jx5AO19X1XQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here you can find the probe body under the `block_sched` section, including four keyword, `position`, `datamodel`, `before` and `after`. More details can be found in Sec. 3.\n",
        "\n",
        "Moreover, we have some metadata like `analyze_hook` which will be referenced later!\n",
        "\n",
        "If you want to use your probe, you can use `-p/--probe` and leave your own path to the probe in TOML format!"
      ],
      "metadata": {
        "id": "aMvzz9taX5lC"
      },
      "id": "aMvzz9taX5lC"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What's the Neutrino Trace?\n",
        "\n",
        "As a versatile system (Sec. 4), Neutrino trace is not just a simple file and we formulate it into a organized \"file system\".\n",
        "\n",
        "First, traces will be placed under `NEUTRINO_TRACEDIR` (configurable via `--trace`), which is default to be `./trace`. By `ls` we can see traces arranged in order:"
      ],
      "metadata": {
        "id": "3ahugz4DX77S"
      },
      "id": "3ahugz4DX77S"
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir(\"block_sched/trace\") # take the simplest block_sched as example"
      ],
      "metadata": {
        "id": "sfCzOLh6X7UA"
      },
      "id": "sfCzOLh6X7UA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each directory corresponding to traces of a run and it's named by the time and pid.\n",
        "For example `Apr24_231539_1860576` means it's a run at:\n",
        "* April 24, 23:15:39, of your local timezone\n",
        "* PID of this process is 1860576\n",
        "\n",
        "The use of `pid` is due to the process-independent design of Neutrino that if your program uses multiple process, there'll be multiple trace dir distinguished by the `pid`.\n",
        "\n",
        "Now we start to explore content of each trace. Probably the latest one."
      ],
      "metadata": {
        "id": "scqOvrl2YEkF"
      },
      "id": "scqOvrl2YEkF"
    },
    {
      "cell_type": "code",
      "source": [
        "# !apt install tree # run this if your system don't have tree\n",
        "tracedir = os.listdir(\"block_sched/trace\")[-1]\n",
        "!tree \"./trace/{tracedir}\" # an undocumented trick"
      ],
      "metadata": {
        "id": "--wb7kxpYGSa"
      },
      "id": "--wb7kxpYGSa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here you can find:\n",
        "* `probe.toml`: a copy of probe code shown above, useful for repeating the experiment.\n",
        "* `event.log`: an important log of all the events captured by Hook Driver (Sec.4.1)\n",
        "* `kernel`: a folder containing all kernels captured and processed by Neutrino.\n",
        "* `result`: a folder containing all traces dumped\n",
        "\n",
        "Now let's deep dive into the `event.log`:"
      ],
      "metadata": {
        "id": "x6UitrhtYL54"
      },
      "id": "x6UitrhtYL54"
    },
    {
      "cell_type": "code",
      "source": [
        "!cat \"block_sched/trace/{tracedir}/event.log\""
      ],
      "metadata": {
        "id": "saktUQfqYNyK"
      },
      "id": "saktUQfqYNyK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's break it down line to line. **Note that pointers will change from run to run, just for illustration**:\n",
        "1. `[pid] 1860576` records the pid of this process\n",
        "2. `[cmd] 26 python block_sched/raw.py` records the command line, useful for classifying traces.\n",
        "3. `[info] dl 0x26aa350` and `[info] init success` records the status of Hook Driver initialization for internal checking.\n",
        "4. `[mem]` records the [memory operations](https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__MEM.html#group__CUDA__MEM), useful for checking illegal memory access. For example, here it states driver sucessfully (return code 0) allocate 33554432 bytes GMEM at ptr 0x7fdf78000000.\n",
        "5. `[mod]` records the [module operations](https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__MODULE.html#group__CUDA__MODULE), useful for checking GPU code. For example, here it states a function named `_ZN2at6native29vectorized_elementwise_kernel...` was first loaded via `cuLibraryLoadData` from a [fatbinary](https://developer.nvidia.com/blog/cuda-pro-tip-understand-fat-binaries-jit-caching/) and lowered via `cuLibraryGetModule` and `cuModuleGetFunction`.\n",
        "6. `[exec] funcmap-not-find` states internal function storage don't find the JIT record. This happens for every first-seen code and will trigger JIT probing.\n",
        "7. `[jit]` records the interaction with probe engines. First it states kernel was `find` from binary storage and was `rename` to SHA1. Next, a folder is created under `kernel` dir and the code was written to `original.bin`. Then a subprocess is forked to launch the probe engine and we wait for the status.\n",
        "8. `[exec]` records the execution of probing engine.\n",
        "9. `[analyze]` records the launch details of analyze scripts that finally print out `No.Blocks...\n",
        "\n",
        "`event.log` records the operations made by the hooked driver, and how about the probe engine?\n",
        "To address this, we need to explore the contents of `kernel/`."
      ],
      "metadata": {
        "id": "-pIB7QJ6YQ-3"
      },
      "id": "-pIB7QJ6YQ-3"
    },
    {
      "cell_type": "code",
      "source": [
        "!ls \"block_sched/trace/{tracedir}/kernel\""
      ],
      "metadata": {
        "id": "DwV_16V3YmSG"
      },
      "id": "DwV_16V3YmSG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each directory under `kernel/` corresponds to a kernel and is named by the SHA1 of kernel name with an indexed prefix for referencing.\n",
        "\n",
        "And under each kernel dir, there will be a `original.bin` dumped by hook driver and all the rest are created by hooked driver.\n",
        "The probe engine follows following steps:\n",
        "1. `objdump original.bin` to extrace assembly into `original.ptx`. Warning: `original.ptx` can be LARGE because many kernels are fused into one file.\n",
        "2. Prune the asm by the kernel name (provided as `sys.argv[-1]`, see above `[exec] subproc`) and save to `pruned.ptx` for checking.\n",
        "3. Probe the asm by the probe in `NEUTRINO_PROBE` envariable and save it to `probed.ptx`\n",
        "4. `assemble` the `pruned.ptx` and `probed.ptx` into `.bin` of binary machine code.\n",
        "5. Write the `kernel.info` for the hooked driver to read the enough metadata for `[exec]`.\n",
        "\n",
        "Moreover, the probed engine will writes all the log into `process.log`, let's take a look first."
      ],
      "metadata": {
        "id": "c7gnpYOsYeoi"
      },
      "id": "c7gnpYOsYeoi"
    },
    {
      "cell_type": "code",
      "source": [
        "kernel_dir = os.listdir(f\"block_sched/trace/{tracedir}/kernel\")[0] # will be 0_xxx\n",
        "!cat \"block_sched/trace/{tracedir}/kernel/{kernel_dir}/process.log\""
      ],
      "metadata": {
        "id": "anGJ237lYQoN"
      },
      "id": "anGJ237lYQoN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's break it from top to down:\n",
        "* First line is the kernel filtering information (set via `--filter`/`--kernel`). It's empty here because we don't set anything.\n",
        "* Second line is the `objdump` information, stating the command used to dump the code.\n",
        "* Third line is the kernel name being used.\n",
        "* Then it's the command and logs of assembler (`ptxas` for NVIDIA). Please pay special attention to the last line stating no.registers used (for producing Table. 2) and constant memroy `cmem[0]` (used for kernel parameters, here Neutrino use 8 bytes more for a 64bit pointer).\n",
        "* Finally is the auto-generated Python code for trace reading conforming Sec. 4.4, helpful for building trace analysis tools, detailed presented in answering the 3rd question.\n",
        "\n",
        "And when the probing engine fails, it will also print out the trace back tree here for analysis, like this:\n",
        "```\n",
        "['']\n",
        "[decompile] via cuobjdump -ptx\n",
        "_ZN2at6native29vectorized_elementwise_kernelILi4ENS0_11FillFunctorIN3c104HalfEEENS_6detail5ArrayIPcLi1EEEEEviT0_T1_\n",
        "Traceback (most recent call last):\n",
        "  File \"/home/root/anaconda3/envs/testenv/lib/python3.11/site-packages/neutrino/probe/cuda.py\", line 747, in <module>\n",
        "    probed_ptx, probe_mem_sizes, trace_reading_code = probing(entry_section, probes)\n",
        "                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
        "  File \"/home/root/anaconda3/envs/testenv/lib/python3.11/site-packages/neutrino/probe/cuda.py\", line 507, in probing\n",
        "    in3:  str = operands[3] if len(operands) >= 4 else None\n",
        "    ^^^\n",
        "UnboundLocalError: cannot access local variable 'out' where it is not associated with a value\n",
        "```\n",
        "\n",
        "We build the probe engine in Python so you can easily debug and extend new functionalities if the current cannot fulfill your need."
      ],
      "metadata": {
        "id": "q_kK4qnRYv-Y"
      },
      "id": "q_kK4qnRYv-Y"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Moreover, `process.log` is for huamn-reading, another log for the hook driver to read back is the `kernel.info`."
      ],
      "metadata": {
        "id": "zgBBLb9DY_1a"
      },
      "id": "zgBBLb9DY_1a"
    },
    {
      "cell_type": "code",
      "source": [
        "!cat \"block_sched/trace/{tracedir}/kernel/{kernel_dir}/kernel.info\""
      ],
      "metadata": {
        "id": "yjOCQcVUYs1F"
      },
      "id": "yjOCQcVUYs1F",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From top to down, `kernel.info` contains:\n",
        "* First line is the kernel name, like `_ZN2at6native29vectorized_elementwise_kernel...`\n",
        "* Second line is the number of original kernel parameters, like 3.\n",
        "* Third line is the number of probes *saving records*, here is 1. Then it's `n=1` lines followed containing the datamodel, of type (`0:=thread/1:=warp`) and bytes, 16 here, saved for each thread/warp.\n",
        "* Later, there'll be a line containing optional analyze hook, like `block_sched.py` here.\n",
        "\n",
        "These will be parsed by the hook driver for execution usage (`[exec]`)."
      ],
      "metadata": {
        "id": "nbcPrde-Y56y"
      },
      "id": "nbcPrde-Y56y"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we can take a look at the difference of `pruned.ptx` and `probed.ptx`:"
      ],
      "metadata": {
        "id": "NyeM2_XiZB7i"
      },
      "id": "NyeM2_XiZB7i"
    },
    {
      "cell_type": "code",
      "source": [
        "!cat \"block_sched/trace/{tracedir}/kernel/{kernel_dir}/pruned.ptx\""
      ],
      "metadata": {
        "id": "H3g5WmTiY7ft"
      },
      "id": "H3g5WmTiY7ft",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat \"block_sched/trace/{tracedir}/kernel/{kernel_dir}/probed.ptx\""
      ],
      "metadata": {
        "id": "3YP4eeRGZD5V"
      },
      "id": "3YP4eeRGZD5V",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we can the global definition are kept and the probe engine make following modification:\n",
        "1. A parameter `.param .u64 param_block_sched` is added\n",
        "2. Probes added at corresponding places, i.e., kernel start and end\n",
        "3. Buffer calculation in `// begin buffer calculation` and `// begin block_sched buffer`"
      ],
      "metadata": {
        "id": "Ap3Asl96ZEVa"
      },
      "id": "Ap3Asl96ZEVa"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What is the analyze code to finally print out result?\n",
        "\n",
        "Upon here, the last question is how the `No.block:32768 Running:602241 Scheduling:87713(cycle)` is analyzed and printed.\n",
        "\n",
        "First, Neutrino will save all raw traces in the `result` directory.\n",
        "Raw traces are of raw binary and is orderly named by the TIME since the driver starts."
      ],
      "metadata": {
        "id": "iqSY0n63ZJWQ"
      },
      "id": "iqSY0n63ZJWQ"
    },
    {
      "cell_type": "code",
      "source": [
        "!ls \"block_sched/trace/{tracedir}/result/\""
      ],
      "metadata": {
        "id": "iRTCY-HfZUTx"
      },
      "id": "iRTCY-HfZUTx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To convert the raw binary into valuable analyzed results, Neutrino provides analyze_hook for user to register in the `probe.toml`.\n",
        "For example, here we register the `block_sched.py` as the analyze hook. Relative paths will be resolved based on `tools/` directory of installation folder."
      ],
      "metadata": {
        "id": "FaE_Ww1yZUks"
      },
      "id": "FaE_Ww1yZUks"
    },
    {
      "cell_type": "code",
      "source": [
        "!cat \"block_sched/trace/{tracedir}/probe.toml\""
      ],
      "metadata": {
        "id": "pXR22pJgZHyS"
      },
      "id": "pXR22pJgZHyS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Real code of block_sched.py\n",
        "!cat {os.path.join(os.path.dirname(neutrino.__file__), \"tools\", \"block_sched.py\")}"
      ],
      "metadata": {
        "id": "QmoDTen5ZRgk"
      },
      "id": "QmoDTen5ZRgk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are two part of the code, separated by `# END OF GENERATED CODE`:\n",
        "* 1st part is the trace reading code auto-generated by Neutrino (see `process.log` above) that are used to read `.bin` traces into Python objects familiar to developers.\n",
        "* 2nd part is the analyze code built upon the `parse` written by developers. Here we simulate a FIFO scheduler based on traces collected, and print and calculate the scheduling times."
      ],
      "metadata": {
        "id": "s8cmvL-FZPrb"
      },
      "id": "s8cmvL-FZPrb"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "testenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "AafFlXQkWgh8",
        "AiMAfcRnXyjV",
        "3ahugz4DX77S",
        "iqSY0n63ZJWQ"
      ]
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}